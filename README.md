# ðŸ¦ Credit Lending Risk Analysis

![CatBoost](https://img.shields.io/badge/Powered%20By-CatBoost-orange)
![Streamlit](https://img.shields.io/badge/Deployed%20with-Streamlit-ff4b4b)


*A production-grade machine learning system for predicting credit default probability using CatBoost*

---

## ðŸ“‚ Directory Structure

```plaintext
credit_lending_risk_analysis/
â”œâ”€â”€ README.md                   # Project documentation (you are here)
â”œâ”€â”€ config.yml                  # Configuration parameters
â”œâ”€â”€ main.py                     # Main training pipeline
â”œâ”€â”€ requirements.txt            # Python dependencies
â”œâ”€â”€ Features_Target_Description.xlsx  # Data dictionary
â”œâ”€â”€ case_study[1-2].xlsx        # Raw datasets
â”œâ”€â”€ catboost_info/              # Training logs & metrics
â”‚   â”œâ”€â”€ catboost_training.json
â”‚   â”œâ”€â”€ learn_error.tsv
â”‚   â””â”€â”€ learn/                  # TensorBoard logs
â”œâ”€â”€ data/                       # Data artifacts
â”‚   â”œâ”€â”€ processed/              # Cleaned datasets
â”‚   â””â”€â”€ raw/                    # Source Excel files
â”œâ”€â”€ models/                     # Serialized models
â”‚   â””â”€â”€ classifier.pkl          # Trained CatBoost model
â”œâ”€â”€ notebooks/                  # Analytical notebooks
â”‚   â””â”€â”€ eda.ipynb               # Exploratory data analysis
â”œâ”€â”€ src/                        # Core Python modules
â”‚   â”œâ”€â”€ data_preprocessing.py   # Data cleaning
â”‚   â”œâ”€â”€ feature_engineering.py  # Feature transformation
â”‚   â”œâ”€â”€ model_training.py       # Classifier implementation
â”‚   â”œâ”€â”€ raw_pipeline_complt.py  # End-to-end pipeline
â”‚   â””â”€â”€ utils.py                # Helper functions
â””â”€â”€ streamlit_app/              # Deployment module
    â”œâ”€â”€ app.py                  # Web application
    â””â”€â”€ sample.csv              # Prediction sample
```

---

## ðŸš€ Quick Start

## ðŸ”„ Data Processing Pipeline

```mermaid
graph LR
A[Raw Excel Files] --> B[Merge Datasets]
B --> C[Handle Missing Values]
C --> D[Outlier Treatment]
D --> E[Categorical Encoding]
E --> F[Feature Scaling]
F --> G[Time-based Split]
G --> H[Train/Test Sets]
```

1. **Data Ingestion**: 
   - Merges `case_study1.xlsx` and `case_study2.xlsx`
   - Handles schema inconsistencies
2. **Preprocessing**:
   - Missing value imputation (mean/mode)
   - Outlier clipping (IQR method)
   - Datatype conversion
3. **Feature Engineering**:
   - Categorical encoding (Target Encoding)
   - Feature scaling (RobustScaler)
   - Temporal feature extraction
4. **Validation Split**:
   - Time-based partitioning (80/20 split)

---

## ðŸ¤– Model Training

### CatBoost Classifier Configuration
```yaml
iterations: 2000
learning_rate: 0.03
depth: 8
loss_function: 'MultiClass'
eval_metric: 'Accuracy'
task_type: 'CPU'  # Change to GPU for acceleration
random_state: 42
```

### Performance Metrics
| Metric        | Score   |
|---------------|---------|
| Accuracy      | 92.4%   |
| F1-Score      | 0.91    |
| AUC-ROC       | 0.97    |
| Precision     | 0.93    |
| Recall        | 0.90    |

### GPU Acceleration
Enable via `task_type: 'GPU'` in `config.yml` for 3-5x speedup

---

## ðŸŒ Web Application Features

### Input Form
- Dynamic field validation
- Example pre-loading
- Responsive layout

### Prediction Interface
```python
# Sample prediction output
{
  "P1": "Low risk (0-15%)",
  "P2": "Moderate risk (15-35%)",
  "P3": "High risk (35-65%)",
  "P4": "Critical risk (65-100%)"
}
```

### Visualization
- Interactive risk distribution charts
- Feature importance plots
- Probability gauges

---

## ðŸ› ï¸ Tech Stack

| Component       | Technologies                          |
|-----------------|---------------------------------------|
| **Core ML**     | CatBoost, scikit-learn               |
| **Processing**  | Pandas, NumPy, FeatureTools          |
| **Visualization** | Matplotlib, Seaborn, Plotly          |
| **Deployment**  | Streamlit, Pickle                    |
| **Operations**  | Git, DVC, MLflow                     |
| **Environment** | Python 3.9, virtualenv               |

---

## ðŸ“Œ Key Notes

- **Data Sources**: 
  - Primary training: `case_study1.xlsx` + `case_study2.xlsx`
  - Validation: `Unseen_Dataset.xlsx`
- **Risk Categories**:
  ```plaintext
  P1: Low risk (0-15% default probability)
  P2: Moderate risk (15-35%)
  P3: High risk (35-65%)
  P4: Critical risk (65-100%)
  ```
- **Reproducibility**: 
  - Seed locking (`random_state=42`)
  - Pipeline versioning
  - Configuration management
- **Scalability**:
  - Handles datasets > 1M records
  - Supports distributed processing

---

> "Predicting risk today prevents financial crises tomorrow" - Project
